\section{ConvNeXt}
ConvNeXt is a Fully Convolutional Neural Network (FCNN) presented in the paper "A ConvNet for the 2020s" \cite{liu2022convnet}. This network is utilized in the KARINA model and, according to its author, outperforms transformers on primary Vision testing metrics. The author details the roadmap used to modernize the architecture and surpass the current best transformer, SwinT, presented in \cite{liu2021swin}.

We will follow the author's roadmap to explain the modernization process and gain a better understanding of ConvNeXt's functionality.

\subsection*{Training Techniques}
The initial hypothesis is that the training procedure is one of the critical aspects to improve for achieving higher accuracy. The author suggests that transformer training procedures have been refined more and thus, applies a similar training procedure for ConvNeXt. Some of the changes include: increasing the number of epochs, using the AdamW optimizer, data augmentation techniques such as Mixup, Cutmix, RandAugment, Random Erasing, and regularization techniques including Stochastic Depth and Label Smoothing.

The author concludes that "a significant portion of the performance difference between traditional ConvNets and vision Transformers may be due to the training techniques."

A summary of the (pre-)training configuration can be found in Table \ref{tab:train_detail},and a summary of the fine-tuning configuration can be found in Table \ref{tab:ft_detail}.\\


\begin{table}[ht]
\tiny
\begin{minipage}{.55\linewidth}
\centering
\begin{tabular}{@{\hskip -0.05ex}l|c@{\hskip 1ex}c}
& ConvNeXt-T/S/B/L & ConvNeXt-T/S/B/L/XL \\
\multirow{2}{*}{(pre-)training config} & ImageNet-1K & ImageNet-22K \\
& 224$^2$ & 224$^2$ \\
\hline
weight init & trunc. normal (0.2) & trunc. normal (0.2) \\
optimizer & AdamW & AdamW\\
base learning rate & 4e-3 & 4e-3 \\
weight decay & 0.05 & 0.05 \\
optimizer momentum & $\beta_1, \beta_2{=}0.9, 0.999$ & $\beta_1, \beta_2{=}0.9, 0.999$ \\
batch size & 4096 & 4096 \\
training epochs & 300 & 90 \\
learning rate schedule & cosine decay & cosine decay \\
warmup epochs & 20 & 5 \\
warmup schedule & linear & linear \\
layer-wise lr decay  & None & None \\
randaugment  & (9, 0.5) & (9, 0.5) \\
mixup  & 0.8 & 0.8 \\
cutmix  & 1.0 & 1.0 \\
random erasing  & 0.25 & 0.25 \\
label smoothing  & 0.1 & 0.1 \\
stochastic depth & 0.1/0.4/0.5/0.5 & 0.0/0.0/0.1/0.1/0.2 \\
layer scale & 1e-6 & 1e-6 \\
head init scale  & None & None \\
gradient clip & None & None \\
exp. mov. avg. (EMA) & 0.9999 & None\\
\end{tabular}
\caption{\textbf{ImageNet-1K/22K (pre-)training settings}. Multiple stochastic depth rates (e.g., 0.1/0.4/0.5/0.5) are for each model (e.g., ConvNeXt-T/S/B/L) respectively.}
\label{tab:train_detail}
\end{minipage}%
\begin{minipage}{.4\linewidth}
\centering
\begin{tabular}{@{\hskip -0.05ex}l@{\hskip 2.6ex}|cc}
\multirow{2}{*}{fine-tuning config} & ImageNet-1K   & ImageNet-1K  \\
 & 384$^2$ & 224$^2$ and 384$^2$ \\
\hline
optimizer & AdamW & AdamW\\
base learning rate & 5e-5 & 5e-5 \\
weight decay & 1e-8 & 1e-8 \\
optimizer momentum & $\beta_1, \beta_2{=}0.9, 0.999$ & $\beta_1, \beta_2{=}0.9, 0.999$ \\
batch size & 512 & 512 \\
training epochs & 30 & 30 \\
learning rate schedule & cosine decay & cosine decay \\
layer-wise lr decay & 0.7 & 0.8  \\
warmup epochs & None & None \\
warmup schedule & N/A & N/A \\
randaugment & (9, 0.5) & (9, 0.5) \\
mixup & None & None \\
cutmix  & None & None \\
random erasing & 0.25 & 0.25 \\
label smoothing  & 0.1 & 0.1 \\
stochastic depth  & 0.8/0.95 & 0.0/0.1/0.2/0.3/0.4 \\
layer scale & pre-trained & pre-trained \\
head init scale & 0.001 & 0.001 \\
gradient clip & None & None \\
exp. mov. avg. (EMA) & None & None(T-L)/0.9999(XL) \\
\end{tabular}
\caption{\textbf{ImageNet-1K fine-tuning settings}. Multiple values (e.g., 0.8/0.95) are for each model (e.g., ConvNeXt-B/L) respectively. }
\label{tab:ft_detail}
\end{minipage}
\end{table}



\newpage
\subsection*{Macro design}

The autors made two main changes to the design wich are \textbf{Changing stage} compute ratio and \textbf{Changing stem to “Patchify”}. We will go trough them.\\

The stages of the computation are a macro design choice. The stage are blocks of operation done in order they often reflect the end goal of the Neural Network. \textit{Changing stage} here means changing the number of time one block is computed at each stage. For reference the old ResNet had a compute factor of [3,4,6,3] after the stem layer and SwinT has a ratio of [1,1,3,1] and for larger transformer [1,1,9,1]. Following this architecture the author proposed a new one following the first ratio [3,3,9,3]. It has improved the accuracy of the model but the author points out that a better design is likely to exist.\\

The stem layer is design to adapt the image for the processing in the network in computer vision there are a lot of redundancy is the image and that is why the stem layer has the role of downsampling the input image. One main difference with transformer is the rate of the downsampling, for convNet the rate is 4 time and SwinT has a rate of 14. The convolution layer of the standard resNet is 7*7 when SwinT has a 4*4. The stem layer of the new convNet is now 4*4 with a stride of 4 (previously the stride was 2) to augment the downsampling. The authors use what the call the “patchify stem” (4×4 non-overlapping convolution) in the network.\\

\subsection*{Grouped convolution}

The idea here was to implement what had been done in other convolutionnal network to have a better trade-off Flops/accuracy. The guiding principle is to “use more groups, expand width”. The idea here is to use depth-wise convolution, a grouped convolution where the number of groups equals the number of channels. The combination of depth-wise conv and 1 × 1 convs leads to a separation of spatial and channel mixing, a property shared by vision Transformers, where each operation either mixes information across spatial or channel dimension, but not both. By doing so the Flops are reduced and they increased the network width to the same number of channels as Swin-T’s (from 64 to 96)
to return to the same amount of Flops.

\subsection*{Inverted Bottleneck}

One major difference between classical ConvNet and Transformer is the use of inverted bottleneck for the latter. Inverted bottleneck is a technique where the "hidden" dimension of the MLP is wider than the input and output. This opperation improved the performance and decrease the Flops which is quite a fun result.

\subsection*{Large Kernel Sizes}

One of the most distinguishing aspects of vision Transformers is their non-local self-attention, which enables each layer to have a global receptive field. For SwinT the self attention layer is has a window size of 7*7 contrary to 3*3 for classical resNet. To explore large kernels, one prerequisite is to move up the position of the depthwise conv layer. In transformers the Multi-Head Self Attention (MSA) Layer is placed ahead of the MLP. They tested multiple size of window and found that the best one was 7×7 depthwise conv in each block.

\subsection*{Micro Design}

The first micro design change that they made was to substituted ReLU activation function with GELU. The second change they made was to have the same number of activation function as the transformer wich is one per MLP. So they have only kept the on GELU function between the two one by  one layer. Again the removed normalisation layer that are less present in transformers. They only kept one BatchNorm (BN) before the one by one conv layer. They now have less normalisation function than the transformers because they empirically found that it works best. They then changed BN for Layer Normalisation (LN) wich is mostly used in transformers and found a slight increase in performance. They used 2*2 conv layer with stride 2 for downsampling between each stage and added LN for stabilising the result (one before each downsampling layer, one after the stem, and one after the final global average pooling).